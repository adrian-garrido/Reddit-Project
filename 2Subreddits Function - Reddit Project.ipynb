{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_subreddits(subreddit1, subreddit2):\n",
    "    url1 = 'https://www.reddit.com/r/' + subreddit1 + '/top.json?sort=top&t=all'\n",
    "    url2 = 'https://www.reddit.com/r/'+ subreddit2 +'/top.json?sort=top&t=all'\n",
    "    \n",
    "    #gathering urls and checking status codes\n",
    "    headers = {'User-agent': 'Adrian'}\n",
    "    res1 = requests.get(url1, headers = headers)\n",
    "    res2 = requests.get(url2, headers = headers)\n",
    "    if (res1.status_code < 300 and res2.status_code < 300):\n",
    "        print ('Urls received')\n",
    "    else:\n",
    "        print('Please inset a valid subreddit')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    print('Gathering posts...')\n",
    "    \n",
    "    \n",
    "    # gathering posts for subreddit 1\n",
    "    posts1 = []\n",
    "    after = None\n",
    "    for i in range(40):\n",
    "        if after == None:\n",
    "            params = {}\n",
    "        else:\n",
    "            params = {'after': after}\n",
    "        res1 = requests.get(url1, params = params, headers = headers)\n",
    "        if res1.status_code == 200:\n",
    "            the_json1 = res1.json()\n",
    "            posts1.extend(the_json1['data']['children'])\n",
    "            after = the_json1['data']['after']\n",
    "        else:\n",
    "            print(f'Server error {res1.status_code}, please try different subreddits')\n",
    "            return\n",
    "        time.sleep(1)\n",
    "    print(f'{len(posts1)} posts gathered from {subreddit1} subreddit')\n",
    "    \n",
    "    \n",
    "    # gathering posts for subreddit 2\n",
    "    posts2 = []\n",
    "    for i in range(40):\n",
    "        if after == None:\n",
    "            params = {}\n",
    "        else:\n",
    "            params = {'after': after}\n",
    "        res2 = requests.get(url2, params = params, headers = headers)\n",
    "        if res2.status_code == 200:\n",
    "            the_json2 = res2.json()\n",
    "            posts2.extend(the_json2['data']['children'])\n",
    "            after = the_json2['data']['after']\n",
    "        else:\n",
    "            print(f'Server error {res2.status_code}, please try different subreddits')\n",
    "            return\n",
    "        time.sleep(1)\n",
    "    print(f'{len(posts2)} posts gathered from {subreddit2} subreddit')\n",
    "    \n",
    "    \n",
    "    # checking number of posts, if too few, return\n",
    "    if (len(posts1) < 20 or len(posts2) < 20):\n",
    "        print('Subreddits do not contain enough posts, please try different subreddits')\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # creating dataframe with data\n",
    "    posts1_df = pd.DataFrame(posts1)\n",
    "    posts2_df = pd.DataFrame(posts2)\n",
    "    \n",
    "    # creating subreddit column. 1 stands for subreddit1 and 0 stands for subreddit2\n",
    "    posts1_df['subreddit'] = 1\n",
    "    posts2_df['subreddit'] = 0\n",
    "    \n",
    "    # getting titles and selftext from posts from the original lists\n",
    "    posts1_df['title'] = [(post['data']['title']) for post in posts1]\n",
    "    posts2_df['title'] = [(post['data']['title']) for post in posts2]\n",
    "    \n",
    "    posts1_df['selftext'] = [(post['data']['selftext']) for post in posts1]\n",
    "    posts2_df['selftext'] = [(post['data']['selftext']) for post in posts2]\n",
    "    \n",
    "    # combining title text and selftext into another column\n",
    "    posts1_df['alltext'] = posts1_df['title'] + ' ' + posts1_df['selftext']\n",
    "    posts2_df['alltext'] = posts2_df['title'] + ' ' + posts2_df['selftext']\n",
    "    \n",
    "    \n",
    "    # concat both dataframes\n",
    "    master_df = pd.concat([posts1_df, posts2_df], ignore_index = True)\n",
    "    try:\n",
    "        master_df.drop('kind', axis = 1, inplace = True) # removing useless column\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # getting rid off urls and mentions of other subreddits\n",
    "    master_df['alltext'] = master_df.alltext.map(lambda x: re.sub('\\s[\\/]?r\\/[^s]+', ' ', x))\n",
    "    master_df['alltext'] = master_df.alltext.map(lambda x: re.sub('http[s]?:\\/\\/[^\\s]*', ' ', x))\n",
    "    master_df['alltext'] = master_df.alltext.map(lambda x: re.sub(\"\\d+\", \"\", x))\n",
    "\n",
    "    \n",
    "    # checking for unbalanced classes:\n",
    "    if (((len(master_df[master_df['subreddit']==1])/(len(master_df['subreddit']))) > .60)\n",
    "        or (len(master_df[master_df['subreddit']==0])/(len(master_df['subreddit']))) > .60):\n",
    "        print('WARNING: Unbalanced classes')\n",
    "        print('Do you wish to continue?')\n",
    "        answer = input()\n",
    "        if answer == 'yes':\n",
    "            pass\n",
    "        else:\n",
    "            print('Function has ended')\n",
    "            return\n",
    "    \n",
    "    # baseline\n",
    "    baseline = max(((len(master_df[master_df[\"subreddit\"] == 1]))/(len(master_df))), \n",
    "                   ((len(master_df[master_df[\"subreddit\"] == 0]))/(len(master_df))))\n",
    "    print(f'Your baseline is: {baseline}')\n",
    "    \n",
    "    # defining X and y\n",
    "    X = master_df['alltext']\n",
    "    y = master_df['subreddit']\n",
    "    \n",
    "    \n",
    "    # train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 42)\n",
    "    \n",
    "    \n",
    "    # Tfidf Vectorizer transformer\n",
    "    tfidf = TfidfVectorizer(stop_words = 'english',\n",
    "                            max_df = .95,\n",
    "                            min_df = 5)\n",
    "    \n",
    "    \n",
    "    # applying tfidf vectorizer to train and test data\n",
    "    train_tfi = tfidf.fit_transform(X_train)\n",
    "    train_df = pd.SparseDataFrame(train_tfi, columns = tfidf.get_feature_names())\n",
    "    train_df.fillna(0, inplace = True)\n",
    "    test_tfi = tfidf.transform(X_test)\n",
    "    test_df = pd.SparseDataFrame(test_tfi, columns=tfidf.get_feature_names())\n",
    "    test_df.fillna(0, inplace=True)\n",
    "    \n",
    "    \n",
    "    # RandomForestClassifier model. Also using GridSearch to look for best hyperparameters\n",
    "    rf = RandomForestClassifier()\n",
    "    params = {'min_samples_split': [12, 25, 40], 'n_estimators': [100, 500]}\n",
    "    gs = GridSearchCV(rf, param_grid=params, return_train_score=True, cv=5)\n",
    "    gs.fit(train_df, y_train)\n",
    "    \n",
    "    # accuracy score for randomforestclassifier\n",
    "    print(f'Accuracy score using Random Forest Classifier model is {gs.score(train_df, y_train)} on the train data')\n",
    "    print(f'Accuracy score using Random Forest Classifier model is {gs.score(test_df, y_test)} on the test data')\n",
    "\n",
    "    \n",
    "    \n",
    "    # CountVectorizer transformer\n",
    "    countvect = CountVectorizer(analyzer = 'word',\n",
    "                                tokenizer = None,\n",
    "                                preprocessor = None,\n",
    "                                stop_words = 'english',\n",
    "                                max_features = 5000)\n",
    "\n",
    "    \n",
    "    # applying countvectorizer to our X variable\n",
    "    X_train_cv = countvect.fit_transform(X_train)\n",
    "    X_test_cv = countvect.transform(X_test)\n",
    "    \n",
    "    \n",
    "    # instantiating and using logistic regression model\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train_cv, y_train);\n",
    "    \n",
    "    lr_score_test = lr.score(X_test_cv, y_test)\n",
    "    lr_score_train = lr.score(X_train_cv, y_train)\n",
    "    \n",
    "    # acuracy score\n",
    "    print(f'Accuracy score using Logistic Regression model is {lr_score_train} on the train data')\n",
    "    print(f'Accuracy score using Logistic Regression model is {lr_score_test} on the test data')\n",
    "          \n",
    "    \n",
    "    # Multinomial Naive Bayes\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_cv, y_train)\n",
    "    \n",
    "    nb_score_train = nb.score(X_train_cv, y_train)\n",
    "    nb_score_test = nb.score(X_test_cv, y_test)\n",
    "    \n",
    "    # accuracy score for multinomialNB\n",
    "    print(f'Accuracy score using Multinomial Naive Bayes model is {nb_score_train} on the train data')\n",
    "    print(f'Accuracy score using Multinomial Naive Bayes model is {nb_score_test} on the test data')    \n",
    "    \n",
    "    \n",
    "    importance = pd.DataFrame(gs.best_estimator_.feature_importances_, train_df.columns, columns = ['importance'])\n",
    "    \n",
    "    print('These were the most important words:')\n",
    "    \n",
    "    print(importance.sort_values(by = 'importance', ascending = False).head(10))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urls received\n",
      "Gathering posts...\n",
      "996 posts gathered from europe subreddit\n",
      "992 posts gathered from canada subreddit\n",
      "Your baseline is: 0.5010060362173038\n",
      "Accuracy score using Random Forest Classifier model is 0.9758551307847082 on the train data\n",
      "Accuracy score using Random Forest Classifier model is 0.8430583501006036 on the test data\n",
      "Accuracy score using Logistic Regression model is 0.9899396378269618 on the train data\n",
      "Accuracy score using Logistic Regression model is 0.8772635814889336 on the test data\n",
      "Accuracy score using Multinomial Naive Bayes model is 0.9738430583501007 on the train data\n",
      "Accuracy score using Multinomial Naive Bayes model is 0.8953722334004024 on the test data\n",
      "These were the most important words:\n",
      "           importance\n",
      "canada       0.166086\n",
      "canadian     0.073750\n",
      "canadians    0.034939\n",
      "toronto      0.021928\n",
      "europe       0.021178\n",
      "trudeau      0.017295\n",
      "eu           0.016250\n",
      "vancouver    0.014354\n",
      "alberta      0.013440\n",
      "ontario      0.011652\n"
     ]
    }
   ],
   "source": [
    "two_subreddits('europe', 'canada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urls received\n",
      "Gathering posts...\n",
      "998 posts gathered from datascience subreddit\n",
      "998 posts gathered from computerscience subreddit\n",
      "Your baseline is: 0.5\n",
      "Accuracy score using Random Forest Classifier model is 0.9926519706078825 on the train data\n",
      "Accuracy score using Random Forest Classifier model is 0.8797595190380761 on the test data\n",
      "Accuracy score using Logistic Regression model is 0.9946559786239145 on the train data\n",
      "Accuracy score using Logistic Regression model is 0.8957915831663327 on the test data\n",
      "Accuracy score using Multinomial Naive Bayes model is 0.9565798263193053 on the train data\n",
      "Accuracy score using Multinomial Naive Bayes model is 0.8977955911823647 on the test data\n",
      "These were the most important words:\n",
      "             importance\n",
      "data           0.140256\n",
      "computer       0.073110\n",
      "cs             0.036351\n",
      "science        0.020794\n",
      "python         0.016612\n",
      "programming    0.016132\n",
      "machine        0.015349\n",
      "analysis       0.013567\n",
      "scientist      0.013211\n",
      "scientists     0.012343\n"
     ]
    }
   ],
   "source": [
    "two_subreddits('datascience', 'computerscience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urls received\n",
      "Gathering posts...\n",
      "989 posts gathered from food subreddit\n",
      "999 posts gathered from healthyfood subreddit\n",
      "Your baseline is: 0.5025150905432596\n",
      "Accuracy score using Random Forest Classifier model is 0.9932930918846412 on the train data\n",
      "Accuracy score using Random Forest Classifier model is 0.9517102615694165 on the test data\n",
      "Accuracy score using Logistic Regression model is 0.9825620389000671 on the train data\n",
      "Accuracy score using Logistic Regression model is 0.9496981891348089 on the test data\n",
      "Accuracy score using Multinomial Naive Bayes model is 0.9738430583501007 on the train data\n",
      "Accuracy score using Multinomial Naive Bayes model is 0.9416498993963782 on the test data\n",
      "These were the most important words:\n",
      "          importance\n",
      "homemade    0.343097\n",
      "ate         0.130141\n",
      "salad       0.034016\n",
      "cake        0.022484\n",
      "salmon      0.013892\n",
      "chef        0.013669\n",
      "pro         0.011803\n",
      "healthy     0.011472\n",
      "lunch       0.010605\n",
      "rice        0.008561\n"
     ]
    }
   ],
   "source": [
    "two_subreddits('food', 'healthyfood')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
